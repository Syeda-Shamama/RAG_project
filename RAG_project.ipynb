{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs6rSA8mtFNLI3kOBx8U2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syeda-Shamama/RAG_project/blob/main/RAG_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieval-Augmented Generation (RAG) for AI-Powered Question Answering**"
      ],
      "metadata": {
        "id": "8jgdcxQjRs59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we are building an AI-powered question-answering system using Retrieval-Augmented Generation (RAG), a technique that enhances AI responses by combining retrieval and generation. Instead of relying solely on a language modelâ€™s internal knowledge, our system first retrieves relevant documents from an external source (a vector database) and then uses that information to generate accurate, context-aware answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "xqErxrF4SUHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Install Required Libraries**"
      ],
      "metadata": {
        "id": "ZdKPr44nSmh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfwRi7t8OyGp"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Required Libraries**"
      ],
      "metadata": {
        "id": "xxzuRcM7S4Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "id": "9fP9HmlYZPzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up API keys**"
      ],
      "metadata": {
        "id": "3Jpo1QkjUUR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "cp = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "import os\n",
        "os.environ ['GOOGLE_API_KEY'] = userdata.get ('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "vfSFQTN4Tc1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up Pinecone Index**\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "68n5C0bQUsRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "index_name = \"pro\"# change if desired\n",
        "cp.create_index(\n",
        "name=index_name,\n",
        "dimension=768,\n",
        "metric=\"cosine\",\n",
        "spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "time.sleep(2)\n",
        "index = cp.Index(index_name)"
      ],
      "metadata": {
        "id": "S6D2FSi9bVE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")  # Change model name\n"
      ],
      "metadata": {
        "id": "DEREHZFPiUwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convert Text into Vectors**"
      ],
      "metadata": {
        "id": "Yoe8WP53VcKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"hello, world!\")"
      ],
      "metadata": {
        "id": "z8bMtrmb0oRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector[:3]"
      ],
      "metadata": {
        "id": "159hiJ8fjDNy",
        "outputId": "00e7a288-ca23-4f6f-dd1d-a118665d162a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207, -0.030764883384108543, -0.03062233328819275]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up Pinecone Vector Store**"
      ],
      "metadata": {
        "id": "HTGhKxLQV42l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "oDwIQ9qbQt1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data save\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]"
      ],
      "metadata": {
        "id": "tNDTaqMarBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Store Documents in Pinecone**"
      ],
      "metadata": {
        "id": "-t6L4ZC0WuRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding to vectorstore\n",
        "from uuid import uuid4\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hzXp_gZPu2",
        "outputId": "de2dc98a-64af-4ef2-a09a-b056d3ef2e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5154cf6f-8b2a-4d04-8a37-8dc151f7e7ca',\n",
              " '156563e9-032c-47cf-9aa1-59829dbc4dd4',\n",
              " '68798b51-4a61-40ac-a3d5-370b48b7df1e',\n",
              " '24641ae1-a9e6-4b9d-bc4b-3f0846280bea',\n",
              " '43ea8830-cffa-4d34-98d5-c6acf48c637f',\n",
              " 'fe4fb3c3-63af-400f-974c-1b800624e3ab',\n",
              " 'f59ee4b5-938d-413e-b6f1-087340c9dd3e',\n",
              " 'b990043e-8508-42bf-b423-1f6beb289746',\n",
              " '23059d4b-7d53-42f1-b6b8-d379183409d8',\n",
              " 'e8a9acf0-08ca-4b10-8d2f-f3c7dc8716e9']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Search for Similar Documents**"
      ],
      "metadata": {
        "id": "T4_LpXJPXKaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Reterive\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRiK_RZNbpnW",
        "outputId": "3bd97f5f-13d5-454d-98e5-df600b3ad508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"There are many things happening in the country, nowdays\",\n",
        "    k=3,\n",
        "    filter={\"source\": \"news\"},\n",
        ")\n",
        "for res,score in results:\n",
        "    print(f\"* SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzELYhJidwzj",
        "outputId": "f7730602-dcb7-4f89-eda2-d2afe815559a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* SIM=0.570734] The stock market is down 500 points today due to fears of a recession. [{'source': 'news'}]\n",
            "* SIM=0.521469] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
            "* SIM=0.500749] Robbers broke into the city bank and stole $1 million in cash. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialize Google Gemini LLM**"
      ],
      "metadata": {
        "id": "FFgMLE7fXhcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "R8_WhkO3pxJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define Function to Answer User Queries**"
      ],
      "metadata": {
        "id": "44my5S2rXu7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user(query: str):\n",
        "\n",
        "    if \"vector_store\" not in globals():\n",
        "        raise ValueError(\"Error: vector_store is not defined. Initialize it before calling this function.\")\n",
        "\n",
        "    results = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "    print(f\"Found {len(results)} results.\")\n",
        "\n",
        "    context_texts = [doc.page_content for doc in results]\n",
        "    context_str = \"\\n\".join(context_texts)\n",
        "\n",
        "    prompt = f\"Answer the user's query: {query}. Use these references to help: {context_str}\n",
        "    final_answer = llm.invoke(prompt)\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "query = \"What is breakfast?\"\n",
        "response = answer_to_user(query)\n",
        "print(response)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6iyKwgoMgZh",
        "outputId": "564b671c-10ff-4590-b913-ec08abb322d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 results.\n",
            "content='Based on the provided text, breakfast is a meal that can include chocolate chip pancakes and scrambled eggs.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-dd7136ad-daa6-46f9-b31a-1edd1c06ee80-0' usage_metadata={'input_tokens': 64, 'output_tokens': 21, 'total_tokens': 85, 'input_token_details': {'cache_read': 0}}\n",
            "Based on the provided text, breakfast is a meal that can include chocolate chip pancakes and scrambled eggs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-O4y-moNxToQ"
      }
    }
  ]
}